# Onde o Modelo HSDA Retorna as MÃ©tricas

## Resumo Executivo

O modelo HSDA retorna as mÃ©tricas de avaliaÃ§Ã£o em **mÃºltiplos locais** durante o processo de teste/avaliaÃ§Ã£o. Este documento explica onde encontrar cada tipo de mÃ©trica.

## 1. Locais de SaÃ­da das MÃ©tricas

### ðŸ“Š **Terminal/Console (SaÃ­da Principal)**
```bash
# Comando de teste padrÃ£o
python tools/test.py \
    configs/bevdet_hsda/bevdet-multi-map-aug-seg-only-6class-hsda.py \
    checkpoints/epoch_20.pth \
    --eval=bbox \
    --out results_hsda.pkl
```

**MÃ©tricas exibidas no terminal:**
- **mAP (mean Average Precision)**: PrecisÃ£o mÃ©dia geral
- **NDS (NuScenes Detection Score)**: Score especÃ­fico do NuScenes
- **AP por classe**: PrecisÃ£o mÃ©dia para cada classe de objeto
- **Erros de TP (True Positive)**: Erros de localizaÃ§Ã£o, orientaÃ§Ã£o, velocidade, etc.

### ðŸ“ **Arquivo PKL (Resultados Brutos)**
```bash
# Especificado pelo parÃ¢metro --out
results_hsda.pkl
```
**ConteÃºdo:**
- PrediÃ§Ãµes brutas para cada amostra
- Bounding boxes 3D detectadas
- Scores de confianÃ§a
- Labels preditas

### ðŸ“ˆ **Arquivo JSON (MÃ©tricas Detalhadas)**
```bash
# Gerado automaticamente em:
work_dirs/[experiment_name]/[timestamp]/metrics_summary.json
```

## 2. Estrutura das MÃ©tricas no NuScenes

### **MÃ©tricas Principais**
```json
{
    "mean_ap": 0.XXXX,           // mAP geral
    "nd_score": 0.XXXX,          // NuScenes Detection Score
    "label_aps": {               // AP por classe
        "car": {
            "0.5": 0.XXXX,       // AP@0.5m
            "1.0": 0.XXXX,       // AP@1.0m
            "2.0": 0.XXXX,       // AP@2.0m
            "4.0": 0.XXXX        // AP@4.0m
        },
        "truck": {...},
        "pedestrian": {...}
    },
    "tp_errors": {               // Erros de True Positives
        "trans_err": 0.XXXX,     // Erro de translaÃ§Ã£o
        "scale_err": 0.XXXX,     // Erro de escala
        "orient_err": 0.XXXX,    // Erro de orientaÃ§Ã£o
        "vel_err": 0.XXXX,       // Erro de velocidade
        "attr_err": 0.XXXX       // Erro de atributos
    }
}
```

## 3. CÃ³digo EspecÃ­fico Onde MÃ©tricas SÃ£o Calculadas

### **No arquivo tools/test.py (linhas 216-221):**
```python
if args.eval:
    eval_kwargs = cfg.get('evaluation', {}).copy()
    eval_kwargs.update(dict(metric=args.eval, **kwargs))
    print(dataset.evaluate(outputs, **eval_kwargs))  # â† MÃ‰TRICAS IMPRESSAS AQUI
```

### **No arquivo mmdet3d/datasets/nuscenes_dataset.py (linhas 483-493):**
```python
# record metrics
metrics = mmcv.load(osp.join(output_dir, 'metrics_summary.json'))
detail = dict()
metric_prefix = f'{result_name}_NuScenes'
for name in self.CLASSES:
    for k, v in metrics['label_aps'][name].items():
        val = float('{:.4f}'.format(v))
        detail['{}/{}_AP_dist_{}'.format(metric_prefix, name, k)] = val

detail['{}/NDS'.format(metric_prefix)] = metrics['nd_score']
detail['{}/mAP'.format(metric_prefix)] = metrics['mean_ap']
```

## 4. Exemplo de SaÃ­da de MÃ©tricas

### **Terminal Output TÃ­pico:**
```
Evaluating bboxes of pts_bbox
+----------+--------+--------+--------+--------+--------+
| AP@0.5   | AP@1.0 | AP@2.0 | AP@4.0 | mAP    | NDS    |
+----------+--------+--------+--------+--------+--------+
| 0.2534   | 0.3421 | 0.4123 | 0.4567 | 0.3661 | 0.4234 |
+----------+--------+--------+--------+--------+--------+

Per-class results:
+------------------+--------+--------+--------+--------+
| Class            | AP@0.5 | AP@1.0 | AP@2.0 | AP@4.0 |
+------------------+--------+--------+--------+--------+
| car              | 0.7234 | 0.8123 | 0.8567 | 0.8901 |
| truck            | 0.4123 | 0.5234 | 0.6123 | 0.6789 |
| pedestrian       | 0.1234 | 0.2345 | 0.3456 | 0.4567 |
+------------------+--------+--------+--------+--------+
```

## 5. Como Salvar e Acessar MÃ©tricas

### **MÃ©todo 1: Redirecionamento de Terminal**
```bash
python tools/test.py \
    configs/bevdet_hsda/bevdet-multi-map-aug-seg-only-6class-hsda.py \
    checkpoints/epoch_20.pth \
    --eval=bbox \
    --out results_hsda.pkl > metricas_resultado.txt 2>&1
```

### **MÃ©todo 2: Parsing do JSON**
```python
import json
import mmcv

# Carregar mÃ©tricas detalhadas
metrics = mmcv.load('work_dirs/experiment/metrics_summary.json')
print(f"mAP: {metrics['mean_ap']:.4f}")
print(f"NDS: {metrics['nd_score']:.4f}")

# MÃ©tricas por classe
for class_name, class_metrics in metrics['label_aps'].items():
    print(f"{class_name} AP@2.0: {class_metrics['2.0']:.4f}")
```

### **MÃ©todo 3: Log Files**
Verificar logs em:
```
work_dirs/[experiment_name]/[timestamp].log
```

## 6. ConfiguraÃ§Ã£o de MÃ©tricas

### **No arquivo de configuraÃ§Ã£o:**
```python
# Em configs/bevdet_hsda/bevdet-multi-map-aug-seg-only-6class-hsda.py
evaluation = dict(
    interval=20,           # Avaliar a cada 20 epochs
    pipeline=eval_pipeline # Pipeline de avaliaÃ§Ã£o
)
```

### **MÃ©tricas DisponÃ­veis:**
- `bbox` - MÃ©tricas de bounding box 3D
- `bboxmap` - MÃ©tricas de bbox + mapas (se disponÃ­vel)
- `mAP` - Mean Average Precision
- `NDS` - NuScenes Detection Score

## 7. InterpretaÃ§Ã£o das MÃ©tricas

### **mAP (Mean Average Precision)**
- **Faixa**: 0.0 - 1.0 (quanto maior, melhor)
- **Significado**: PrecisÃ£o mÃ©dia em diferentes thresholds de distÃ¢ncia
- **Bom resultado**: > 0.4 para modelos de referÃªncia

### **NDS (NuScenes Detection Score)**
- **Faixa**: 0.0 - 1.0 (quanto maior, melhor)
- **Significado**: Score composto considerando detecÃ§Ã£o + erros
- **Bom resultado**: > 0.45 para modelos competitivos

### **AP por DistÃ¢ncia**
- **AP@0.5m**: DetecÃ§Ãµes muito precisas
- **AP@1.0m**: DetecÃ§Ãµes precisas
- **AP@2.0m**: DetecÃ§Ãµes moderadamente precisas
- **AP@4.0m**: DetecÃ§Ãµes com tolerÃ¢ncia maior

## 8. Troubleshooting Comum

### **Problema: MÃ©tricas nÃ£o aparecem**
```bash
# Verificar se --eval estÃ¡ especificado
python tools/test.py config.py checkpoint.pth --eval=bbox
```

### **Problema: Arquivo de mÃ©tricas nÃ£o encontrado**
```bash
# Verificar se evaluation estÃ¡ configurado no config
evaluation = dict(interval=20, pipeline=eval_pipeline)
```

### **Problema: MÃ©tricas muito baixas**
- âœ… Verificar se o modelo foi treinado adequadamente
- âœ… Verificar se os dados de teste estÃ£o corretos
- âœ… Verificar se as transformaÃ§Ãµes de coordenadas estÃ£o corretas

## 9. Scripts de AutomaÃ§Ã£o

### **Script para Extrair MÃ©tricas:**
```bash
#!/bin/bash
# run_evaluation.sh

CONFIG="configs/bevdet_hsda/bevdet-multi-map-aug-seg-only-6class-hsda.py"
CHECKPOINT="checkpoints/epoch_20.pth"
OUTPUT_DIR="results/$(date +%Y%m%d_%H%M%S)"

mkdir -p $OUTPUT_DIR

python tools/test.py $CONFIG $CHECKPOINT \
    --eval=bbox \
    --out $OUTPUT_DIR/predictions.pkl \
    > $OUTPUT_DIR/metrics.txt 2>&1

echo "MÃ©tricas salvas em: $OUTPUT_DIR/metrics.txt"
```

---

## âœ… Resumo Final

**As mÃ©tricas do modelo HSDA sÃ£o retornadas em:**

1. **ðŸ–¥ï¸ Terminal** - ExibiÃ§Ã£o imediata durante teste
2. **ðŸ“ arquivo.pkl** - PrediÃ§Ãµes brutas (parÃ¢metro `--out`)
3. **ðŸ“Š metrics_summary.json** - MÃ©tricas detalhadas estruturadas
4. **ðŸ“ Log files** - Logs completos do experimento

**Para acessar rapidamente:**
```bash
# Ver mÃ©tricas no terminal
python tools/test.py config.py checkpoint.pth --eval=bbox

# Salvar mÃ©tricas em arquivo
python tools/test.py config.py checkpoint.pth --eval=bbox > metrics.txt
```
